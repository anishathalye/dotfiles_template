[ -f ~/.env ] && source ~/.env
[ -f ~/.env_private ] && source ~/.env_private

unalias  fd >/dev/null 2>/dev/null # in order to use https://github.com/sharkdp/fd

##########################
# DOTFILES UPDATE
##########################
alias dotfiles="cd $DOTFILES_PATH"
# update dotfiles
alias dotf_update='dotfiles && git pull && bash install; cd -'

##########################
# UPDATE AppImages
##########################
alias update_nextcloud="wget https://download.nextcloud.com/desktop/releases/Linux/latest -O $HOME/bin/Nextcloud.AppImage && chmod +x $HOME/bin/*.AppImage"
alias update_joplin='curl -s https://api.github.com/repos/laurent22/joplin/releases/latest | grep "browser_download_url.*AppImage" | cut -d : -f 2,3 | tr -d \" | wget -qi - -O $HOME/bin/Joplin.AppImage && chmod +x $HOME/bin/*.AppImage'
alias update_etcher='curl -s https://api.github.com/repos/balena-io/etcher/releases/latest | grep "browser_download_url.*x64.AppImage" | cut -d : -f 2,3 | tr -d \" | wget -qi - -O $HOME/bin/balenaEtcher.AppImage && chmod +x $HOME/bin/*.AppImage'
alias update_shotcut='curl -s https://api.github.com/repos/mltframework/shotcut/releases/latest | grep "browser_download_url.*AppImage" | cut -d : -f 2,3 | tr -d \" | wget -qi - -O $HOME/bin/Shotcut.AppImage && chmod +x $HOME/bin/*.AppImage'
alias update_appimages='update_nextcloud; update_joplin; update_etcher; update_shotcut'

alias shotcut='Shotcut.AppImage --QT_SCALE_FACTOR 1'
alias Shotcut='shotcut'

##########################
# ls and cd aliases
##########################
# quick way to get out of current directory
alias ..='cd ..'
alias ...='cd ../../../'
alias ....='cd ../../../../'
alias .....='cd ../../../../'
alias .4='cd ../../../../'
alias .5='cd ../../../../..'
alias cc='ls -f | wc -l' # ls -f does not stat() and makes it faster

path_to_lsd=$(which lsd 2>/dev/null)
if [ -x "$path_to_lsd" ] ; then
    alias ls='lsd'
    alias l='ls -l'
    alias ll='ls -l'
    alias la='ls -a'
    alias lla='ls -la'
    alias lt='ls --tree'
fi


##########################
# CHEF cooking + Ansible provisionning
##########################
alias chef_cd="cd $CHEF_DIR"
alias chef_private_cd="cd $CHEF_DIR/../chef-private"
alias talend="chef_cd && ./bin/talend7-pipeline.sh"
alias chef_imos="chef_cd && rm -f private; ln -s ../chef-private private"
alias chef_pipeline_box_start="chef_cd &&  ./bin/pipeline-box.sh"
alias chef_pipeline_box_provision="chef_cd && vagrant provision pipeline"
alias chef_sample="chef_cd && rm -f private; ln -s private-sample private"
alias vagrant_ssh='var=$(find $CHEF_DIR/nodes/ -type f -name "*\.json" -exec basename {} \; | cut -d . -f 1 | fzf) && cd $CHEF_DIR && vagrant ssh $var'
alias chef_pipeline_bckp_json='cp $CHEF_DIR/private-sample/nodes/pipeline.json $CHEF_DIR/private-sample/nodes/pipeline.json.bckp'
alias chef_pipeline_restore_json='cp $CHEF_DIR/private-sample/nodes/pipeline.json.bckp  $CHEF_DIR/private-sample/nodes/pipeline.json'

alias ansible_provision_latop='/usr/bin/ansible-playbook -i hosts $ANSIBLE_PATH/local.yml -K'
alias ansible_provision_remote='ansible-playbook -i hosts $ANSIBLE_PATH/remote.yml'

##########################
# IMOS related - pipeline 
##########################
# pipeline 
export PIPELINE_TASK_LOG_PATH=/mnt/ebs/log/pipeline/process
alias pipeline_log_tailf='var=$(find /mnt/ebs/log/pipeline -type f -name "*.log" | fzf) && tail -f $var'
alias pipeline_log_tailf_task='var=$(find $PIPELINE_TASK_LOG_PATH -type f -name "task*.log" | fzf) && tail -f $var'
alias pipeline_ls_error_dir='var=$(find $ERROR_DIR -maxdepth 1 -type d | fzf) && ls $var'
alias pipeline_cd_error_dir='var=$(find $ERROR_DIR -maxdepth 1 -type d | fzf) && cd $var'

# retrieve all logs for any UUID in a pipeline task log
# $1 UUID value
function pipeline_log_uuid(){
    local uuid=$1;

    local log_file_path=$(find $PIPELINE_TASK_LOG_PATH -type f -name "task*.log" | fzf)
    grep $uuid $log_file_path
}

# retrieve specific log output for failed file in pipeline
# $1 error file path
function pipeline_log_error_file(){
    # list files in ALL $ERROR_DIR with a uuid at the end
    local uuid_regexp='[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}'
    local list_all_error_files=$(find $ERROR_DIR -type f)
    local file_input=$(echo $list_all_error_files | grep -E .$uuid_regexp |fzf)

    if [ -z "$file_input" ]
        then
        echo "no file selected"
        return 1
    fi

    local uuid_file=`echo "$file_input" | grep -o -E $uuid_regexp`
    local pipeline_name=`basename $(dirname $(readlink -f $file_input))`
    
    # find uuid entries in corresponding pipeline task log
    sed -E "0,/$uuid_file/d;/IncomingFileStateManager.*FILE_(SUCCESS|IN_ERROR)/q" $PIPELINE_TASK_LOG_PATH/tasks.$pipeline_name.log
}

alias talend_cd='var=$(find /mnt/ebs/talend/jobs -maxdepth 1 -type d | fzf) && cd $var'


# wget for imos buck by simply giving the relative path of an object without the full bucket address
# $1 imos bucket url
wget_imos_bucket() {
    local url_suffix=$1; shift;
    local url_prefix="https://s3-ap-southeast-2.amazonaws.com/imos-data/"
    wget ${url_prefix}${url_suffix}
}

# compliance checker running the cf check
# $1 cf version
# $2 netcdf file
cc_cf() {
    local cf_version=$1; shift
    local ncf=$1; shift
    [ -f /mnt/ebs/pipeline/bin/activate ] && . /mnt/ebs/pipeline/bin/activate
    compliance-checker -t=cf:$cf_version $ncf
}

# compliance checker running the cf 1.6 check
# $1 netcdf file
cc_cf_1.6() {
    local ncf=$1; shift
    cc_cf 1.6 $ncf
}

# compliance checker running the latest cf check
# $1 netcdf file
cc_cf_latest() {
    local ncf=$1; shift
    cc_cf latest $ncf
}

# compliance checker running the imos check
# $1 netcdf file
cc_imos() {
    local ncf=$1; shift
    [ -f /mnt/ebs/pipeline/bin/activate ] && . /mnt/ebs/pipeline/bin/activate
    compliance-checker -t=imos $ncf
}

##########################
## github cli
##########################
export GIT_USER=`git config --list | grep user.name| cut -d = -f2`
alias g_issue_list='gh issue list -a $GIT_USER -R aodn/po-backlog && gh issue list -a $GIT_USER -R aodn/content'

##########################
# Display
##########################
alias display_docked_work=". $HOME/.screenlayout/work_screen2_thunder.sh && mouse_setting.sh"
alias work_workspace='display_docked_work & tmux'

alias display_docked_home=". $HOME/.screenlayout/i3_work.sh && mouse_setting.sh"
alias home_workspace='display_docked_home & tmux'

alias display_undocked=". $HOME/.screenlayout/i3_laptop_single_screen.sh && mouse_setting.sh"

alias screen_layout='arandr'
alias brightness_up='sudo light -A 10'
alias brightness_down='sudo light -U 10'
alias brightness_chgrp_sys='/usr/bin/ansible-playbook -i hosts $ANSIBLE_PATH/local.yml -K -t i3'

##########################
# Various
##########################
alias project_officer_user='sudo -u projectofficer -s'
alias rsrc='source ~/.bashrc'
alias clear_tmp="find /tmp -mtime +1 -and -not -exec fuser -s {} ';' -and -exec rm -Rf {} ';'"
alias fzfp="fzf -m --preview 'head -100 {}'"
alias g='git'
alias geoserver_cd="cd $CHEF_DIR/src/geoserver"
alias github_cd="cd $GITHUB_DIR"
alias sublime='subl'
alias grep='grep --color=auto'
alias trim="sed 's/^ *//;s/ *$//'"
alias egrep='egrep --color=auto'
alias fgrep='fgrep --color=auto'
alias copy='| xclip -sel clip'
alias automount='udiskie &'
alias update_linux='sudo mintupdate-launcher'

alias irssi='screen irssi'
alias tmux_start="[[ ! $TERM =~ screen ]] && [ -z $TMUX ] && exec tmux -2 attach"
alias cleanup_cache="brew cleanup && conda clean -a"

alias vita_dock="mplayer tv:// -tv driver=v4l2:device=/dev/video2:width=960:height=544"

function x(){
    if [ -f $1 ] ; then
            case $1 in
                    *.tar.bz2)   tar xvjf $1    ;;
                    *.tar.gz)    tar xvzf $1    ;;
                    *.bz2)       bunzip2 $1     ;;
                    *.rar)       unrar x $1     ;;
                    *.gz)        gunzip $1      ;;
                    *.tar)       tar xvf $1     ;;
                    *.tbz2)      tar xvjf $1    ;;
                    *.tgz)       tar xvzf $1    ;;
                    *.zip)       unzip $1       ;;
                    *.Z)         uncompress $1  ;;
                    *.7z)        7z x $1        ;;
                    *)           echo "Unable to extract '$1'" ;;
            esac
    else
            echo "'$1' is not a valid file"
    fi
}


##########################
# NETWORK
##########################
alias iftop_wlan='iftop -i wlan0'
alias lynx='lynx -cfg=~/.lynx.cfg'
alias elinks_google='elinks www.google.com.au'
alias mac_wlan_random='sudo ifconfig wlp1s0 down && sudo macchanger -r wlp1s0 && sudo ifconfig wlp1s0 up'
alias ports='netstat -tulanp' # open ports
alias ssh-add='ssh-add -t 10h'
alias wifi_restart='sudo systemctl restart wpa_supplicant.service'
alias wifi_connect_to='nmtui'
alias wifi_configure='nm-applet'

if command -v dig > /dev/null; then
    alias myip="dig +short myip.opendns.com @resolver1.opendns.com"
    alias mygeo="curl -w \"\\n\" http://api.hackertarget.com/geoip/\?q=\`myip\`"
else
    alias myip="curl ifconfig.me"
fi

# vpn
alias s='var=$(grep -e "^Host " ~/.ssh/config | sed "s#Host ##" | fzf) && ssh $var' # ssh with fzf to list known hosts from ssh/config
[ -f ~/.utas_vpnc.conf ] && alias vpn_utas_connect='sudo vpnc ~/.utas_vpnc.conf --local-port 0' && alias vpn_utas_disconnect='sudo vpnc-disconnect'

alias vpn_bfunk_start='sudo openvpn --cd $DOTFILES_PRIVATE_PATH/openvpn/ --writepid /run/openvpn/home.pid --config bfunk.conf'
alias vpn_evry_start='sudo openvpn --cd $DOTFILES_PRIVATE_PATH/openvpn/ --writepid /run/openvpn/home.pid --config evry.conf'
alias wg_bfunk_up='wg-quick up bfunk'
alias wg_bfunk_down='wg-quick down bfunk'
alias wg_bfunk_restart='wg_bfunk_down && wg_bfunk_up'

##########################
# vim nvim alias
##########################
if command -v nvim > /dev/null; then
    alias vim='nvim'
    alias vi='nvim'
    alias v='f -e nvim' # replace default fasd alias from zsh plugin
fi
alias vim_plugin_install="vim +'PlugInstall --sync' +'PlugUpdate' +qa"

if command -v bat > /dev/null; then alias cat="bat -p"; fi

##########################
# timezone
##########################
alias tz_paris='TZ="Europe/Paris" date'
alias tz_hobart='TZ="Australia/Hobart" date'
alias tz_utc='TZ="UTC" date'

##########################
# SYSTEM
##########################
if command -v htop > /dev/null; then alias top='htop'; fi
if command -v dfc > /dev/null; then alias df='dfc'; fi

if command -v ncdu > /dev/null; then
    alias gt5=ncdu
    alias hdd_report=ncdu
fi

alias temperature='sensors;sudo hddtemp /dev/sda'
alias shutdown_now='sudo shutdown -h now'
alias latest_installed_pckg='cat /var/log/apt/history.log | grep "\ install\ "'
alias killall='killall -SIGUSR1'
alias lock_screen='gnome-screensaver-command --lock || xscreensaver-command -lock'

fkill() {
    local pid 
    if [ "$UID" != "0" ]; then
        pid=$(ps -f -u $UID | sed 1d | fzf -m | awk '{print $2}')
    else
        pid=$(ps -ef | sed 1d | fzf -m | awk '{print $2}')
    fi  

    if [ "x$pid" != "x" ]
    then
        echo $pid | xargs kill -${1:-9}
    fi  
}

#########################
# DATABASE
#########################
if command -v pgcli > /dev/null; then
    alias psql=pgcli
fi

# list hosts with fzf from pgpass file and connect to DB. 
[ -f $HOME/.pgpass ] && export PGPASS_PATH=$HOME/.pgpass && alias psql-connect='var=$(cat $PGPASS_PATH | cut -d : -f1 | grep -v "^#" | fzf) && psql -d `cat $PGPASS_PATH  | grep $var | cut -d':' -f3 | head -1` -h $var -U `cat $PGPASS_PATH  | grep $var | cut -d':' -f4 | head -1`'

##########################
# file handling
##########################
remove_space_from_filename_recur() {
    find . -depth -name "* *" -execdir rename 's/ /_/g' "{}" \;
}

remove_file_extension() {
    local file="$1"; shift
    mv $file ${file%\.[^.]*}
}

##########################
# NetCDF
##########################
ncdumph() {
    command ncdump -h "$@" | less;
}

###########################
# MUSIC
###########################
alias beet_genre_uniq_update="beet ls -f '\$genre' | sort -u > ~/.beets_genre"
alias split_flac_album='IFS=$(echo -en "\n\b"); for CUE in `find . -type f -name "*.cue"`; do FILE_TO_CONVERT=`basename ${CUE%.*}.flac` && shnsplit -f "$CUE" -t %n-%t -o flac "$FILE_TO_CONVERT" && rm -f "$FILE_TO_CONVERT";done'
alias split_ape_album='IFS=$(echo -en "\n\b"); for CUE in `find . -type f -name "*.cue"`; do FILE_TO_CONVERT=`basename ${CUE%.*}.ape` && shnsplit -f "$CUE" -t %n-%t -o flac "$FILE_TO_CONVERT" && rm -f "$FILE_TO_CONVERT";done'
alias split_wv_album='IFS=$(echo -en "\n\b"); for CUE in `find . -type f -name "*.cue"`; do FILE_TO_CONVERT=`basename ${CUE%.*}.vw` && shnsplit -f "$CUE" -t %n-%t -o flac "$FILE_TO_CONVERT" && rm -f "$FILE_TO_CONVERT";done'
alias music_player='mp3blaster'
alias volume='pavucontrol'

###########################
# paragliding
###########################
alias flightrecorder='cd $HOME/paragliding ; sudo flightrecorder && chown $USER:$USER *.IGC; rsync -avc $HOME/paragliding/ $HOME/Nexctloud/paragliding_igc'
alias igc_viewer='gpligc'
alias riverHeight='python $GITHUB_DIR/River_Height_Tasmania/riverHeight.py && /usr/bin/elinks ~/Nexctloud/Public/RiverStatusTasmania.html'
alias gpsdump_flymaster='cd $HOME/Downloads; ./gpsdump -gy -ca0 -lflight'
alias paragliding_create_osm_map='$GITHUB_DIR/OSM_Tile_Download/osm_ogie3d.py'

##############################
# PYTHON
##############################
python_venv_init () {
    local venv_name=$1; shift
    mkdir venv_name && cd venv_name;
    sourve venv/vin/activate
    xargs -a requirements.txt -n 1 pip install
}
alias pip_install_requirements='xargs -a requirements.txt -n 1 pip install'

kill_py () {
    local python_script_name=$1; shift
    kill -9 $(ps aux | grep '[p]ython.*${python_script_name}\.py'| awk '{print $2}')
}


##############################
# COMICS specific functions
##############################
function pdf2cbz(){
    IFS=$(echo -en "\n\b");
    PDF_FILE="$1"; shift;

    PDF_FILE_PATH=`readlink -f "$PDF_FILE"`;
    PDF_FILE_NAME=`basename "$PDF_FILE_PATH"`;
    PDF_FILE_DIRNAME=`dirname "$PDF_FILE_PATH"`;

    temp_dir=`mktemp -d`;

    if command -v pdfimages > /dev/null; then
        pdfimages -j "$PDF_FILE_PATH" $temp_dir/page && \
        zip -jr "${PDF_FILE_PATH%.*}.cbz" $temp_dir/* && \
        mv "$PDF_FILE_PATH" "$PDF_FILE_DIRNAME"/."$PDF_FILE_NAME" && \
        rm -Rf $temp_dir;
    else
        echo "Please install pdfimages package"
    fi
}
alias comic_pdf2cbz='pdf2cbz'


function pdf2cbz_all_files_in_dir() {
    IFS=$(echo -en "\n\b");
    for f in `find . -maxdepth 1 -type f -name '*.pdf' -not -path '*/\.*' -exec readlink -f {} \;`;do
        pdf2cbz "$f";
    done;
}
alias comic_pdf2cbz_all_files_in_dir=pdf2cbz_all_files_in_dir


function dirs2cbz() {
    IFS=$(echo -en "\n\b");
    for dir in `find . -mindepth 1 -type d -not -empty`; do
        if find "$dir" -maxdepth 1 -type f -not -ipath '.*\.cbz' -not -ipath '.*\.cbr' -not  -ipath '.*\.pdf'| read f; then
            zip "$(dirname "$dir")/$(basename "$dir").cbz" "$dir"/* && rm -Rf "$dir" ;*/
        fi;
    done
}
alias comic_dirs2cbz=dirs2cbz


function append_folder_name_as_prefix_to_file(){
    IFS=$(echo -en "\n\b");
    local file="$1"; shift;
    mv "${file}" "${PWD##*/} - ${file}"
}
alias comic_add_serie_name='append_folder_name_as_prefix_to_file'

function cbz2pdf(){
    IFS=$(echo -en "\n\b");
    ARCHIVE_FILE="$1"; shift;

    ARCHIVE_FILE_PATH=`readlink -f "$ARCHIVE_FILE"`;
    ARCHIVE_FILE_NAME=`basename "$ARCHIVE_FILE_PATH"`;
    ARCHIVE_FILE_DIRNAME=`dirname "$ARCHIVEFILE_PATH"`;

    temp_dir=`mktemp -d`;

    if ! command -v dwebp > /dev/null; then
        echo "Please install webp package"
        return 1
    fi

    if command -v convert > /dev/null; then
        unzip -j "$ARCHIVE_FILE_PATH" -d $temp_dir
        find $temp_dir/ -name "*.webp" -exec dwebp {} -o {}.jpg \;
        convert $temp_dir/*.jpg "${ARCHIVE_FILE_PATH%.*}.pdf"

        mv "$ARCHIVE_FILE_PATH" "$ARCHIVE_FILE_DIRNAME"/."$ARCHIVE_FILE_NAME" && \
        rm -Rf $temp_dir;
    else
        echo "Please install imagemagick package"
        return 1
    fi
}
alias comic_cbz2pdf='cbz2pdf'

function comic_repack_corrupt_archive2cbz(){
    IFS=$(echo -en "\n\b");
    ARCHIVE_FILE="$1"; shift;

    ARCHIVE_FILE_PATH=`readlink -f "$ARCHIVE_FILE"`;
    ARCHIVE_FILE_NAME=`basename "$ARCHIVE_FILE_PATH"`;
    ARCHIVE_FILE_DIRNAME=`dirname "$ARCHIVEFILE_PATH"`;

    if ! command -v arepack > /dev/null; then
        echo "Please install atool package"
        return 1
    fi

    if [[ "$ARCHIVE_FILE" = *.cbz ]]; then
        arepack "$ARCHIVE_FILE_PATH" "${ARCHIVE_FILE_PATH%.*}.rar"
        mv -f "$ARCHIVE_FILE_PATH" "$ARCHIVE_FILE_DIRNAME"/."$ARCHIVE_FILE_NAME" # archive original 

        arepack "${ARCHIVE_FILE_PATH%.*}.rar" "${ARCHIVE_FILE_PATH%.*}.zip" # reconvert to zip
        rm -f "${ARCHIVE_FILE_PATH%.*}.rar"
        mv -f "${ARCHIVE_FILE_PATH%.*}.zip"  "${ARCHIVE_FILE_PATH%.*}.cbz"
    elif [[ "$ARCHIVE_FILE" = *.cbr ]]; then
        arepack "$ARCHIVE_FILE_PATH" "${ARCHIVE_FILE_PATH%.*}.zip"
        mv -f "$ARCHIVE_FILE_PATH" "$ARCHIVE_FILE_DIRNAME"/."$ARCHIVE_FILE_NAME" && \
          mv -f "${ARCHIVE_FILE_PATH%.*}.zip"  "${ARCHIVE_FILE_PATH%.*}.cbz"
    else
        echo "unknown extension"
    fi

    # remove file from archive
    zip -o -d "${ARCHIVE_FILE_PATH%.*}.cbz" \*crg.gif \*ner0.jpg \*ner0_lee.jpg
}


function comic_repack_all_corrupt_archive2cbz(){
    IFS=$(echo -en "\n\b");
    for f in `find . -maxdepth 1 -type f  -not -path '*/\.*' -exec readlink -f {} \;`;do
        comic_repack_corrupt_archive2cbz "$f";
    done;
}


##############################
# docker compose
##############################
alias dcp='docker-compose -f /opt/docker-compose.yml '
alias dcpull='docker-compose -f /opt/docker-compose.yml pull --parallel'
alias dclogs='docker-compose -f /opt/docker-compose.yml logs -tf --tail="50" '
alias dtail='docker logs -tf --tail="50" "$@"'
DOCKER_CONFIG_DIR=/opt/docker_config


##############################
## BFUNK
##############################
alias mount_games='mount_point=games;mkdir -p $HOME/bfunk/$mount_point; sudo mount -t nfs 10.1.1.198:/export/$mount_point $HOME/bfunk/$mount_point/'
alias mount_torrent='mount_point=torrent;mkdir -p $HOME/bfunk/$mount_point; sudo mount -t nfs 10.1.1.198:/export/$mount_point $HOME/bfunk/$mount_point/'
export dd1=/srv/dev-disk-by-label-media_center/
export dd2=/srv/dev-disk-by-id-ata-WDC_WD15EADS-00P8B0_WD-WMAVU0549888-part1/
alias myip='dig +short myip.opendns.com @resolver1.opendns.com'
#alias start_vpn_torrent='sudo systemctl start vpn_access.service; sudo service transmission-daemon start; myip'
#alias stop_vpn_torrent='sudo service transmission-daemon stop; sudo systemctl stop vpn_access.service; myip'
#alias nginx_maintenance='sudo rm -f /etc/nginx/sites-enabled/* && sudo ln -s /etc/nginx/sites-available/omv_maintenance /etc/nginx/sites-enabled/omv_maintenance && sudo service nginx restart;'
#alias nginx_organizr='sudo rm -f /etc/nginx/sites-enabled/* && sudo ln -s /etc/nginx/sites-available/organizr /etc/nginx/sites-enabled/organizr && sudo service nginx restart;'
#alias filebot_movie_rename_dryrun='IFS=$(echo -en "\n\b"); for dir in `find -type d`; do filebot -rename $dir --db TheMovieDB --format "/export/torrent/{plex}" --action test; done'
#alias filebot_movie_rename_move='IFS=$(echo -en "\n\b"); for dir in `find -type d`; do filebot -rename $dir --db TheMovieDB --format "/export/torrent/{plex}" --action move; done'
#alias filebot_tvshows_rename_dryrun='filebot -rename -r -non-strict . --db TheTVDB --format "/export/torrent/{plex}" --action test'
#alias filebot_tvshows_rename_move='filebot -rename -r -non-strict . --db TheTVDB --format "/export/torrent/{plex}" --action move'

AIRSONIC_DATA_DIR=$DOCKER_CONFIG_DIR/airsonic
NEXCTLOUD_DATA_DIR=/export/nexctloud
alias backup_airsonic='sudo service airsonic stop && tar -cvf "/tmp/airsonic_$(date +%F_%R).tar" /var/airsonic && sudo service airsonic restart'
alias backup_nextcloud='sudo docker stop nextcloud && tar -cvf "/tmp/nextcloud_$(date +%F_%R).tar" $NEXCTLOUD_DATA_DIR &&; sudo docker start service nextcloud'

# when files are copied manually to nextcloud folder
alias nextcloud_force_rescan='docker exec nextcloud sudo -u abc php /config/www/nextcloud/occ files:scan --all'
